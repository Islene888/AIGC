import pandas as pd
import urllib.parse
from sqlalchemy import create_engine, text
from datetime import datetime, timedelta
import os

# ===== æ•°æ®åº“è¿æ¥ =====
def get_engine():
    password = urllib.parse.quote_plus("flowgpt@2024.com")
    db_url = f"mysql+pymysql://bigdata:{password}@3.135.224.186:9030/flow_ab_test?charset=utf8mb4"
    return create_engine(db_url, connect_args={"connect_timeout": 600})

# ===== å•æ—¥ç•™å­˜åˆ†æä¸»é€»è¾‘ =====
def run_retention_analysis(engine, experiment_id, target_date):
    next_day = (datetime.strptime(target_date, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y-%m-%d")

    with engine.connect() as conn:
        print(f"\nğŸ“… å¼€å§‹åˆ†ææ—¥æœŸï¼š{target_date}")
        for tbl in ["tmp_user_variation_country", "tmp_active_user_with_d1"]:
            try:
                conn.execute(text(f"DROP TABLE IF EXISTS {tbl}"))
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†è¡¨ {tbl} å¤±è´¥ï¼š", e)

        # ========== 1. åˆ†ç»„ä¿¡æ¯ ==========
        print("ğŸ§© åˆ›å»ºåˆ†ç»„è¡¨ï¼ˆå»é‡ user_idï¼Œä»…ä¿ç•™æœ€æ–° variationï¼‰")
        conn.execute(text(f"""
            CREATE TABLE  tmp_user_variation_country AS
            SELECT /*+ SET_VAR(query_timeout = 120000) */
                   user_id, CAST(variation_id AS CHAR) AS variation
            FROM (
                SELECT user_id, variation_id,
                       ROW_NUMBER() OVER (PARTITION BY user_id ORDER BY event_date DESC) AS rn
                FROM flow_wide_info.tbl_wide_experiment_assignment_hi
                WHERE experiment_id = '{experiment_id}'
                  AND event_date <= '{target_date}'
            ) t
            WHERE rn = 1;
        """))

        # ========== 2. ç•™å­˜ç”¨æˆ·å®½è¡¨ ==========
        print("ğŸ“… æ„å»ºæ´»è·ƒç”¨æˆ·ç•™å­˜å®½è¡¨")
        conn.execute(text(f"""
            CREATE TABLE  tmp_active_user_with_d1 AS
            SELECT DISTINCT
                   a.user_id, a.active_date,
                   CASE WHEN b.user_id IS NOT NULL THEN 1 ELSE 0 END AS is_d1
            FROM (
                SELECT user_id, active_date
                FROM flow_wide_info.tbl_wide_active_user_app_info
                WHERE active_date = '{target_date}'
                  AND keep_alive_flag = 1 AND user_id != ''
            ) a
            LEFT JOIN (
                SELECT user_id
                FROM flow_wide_info.tbl_wide_active_user_app_info
                WHERE active_date = '{next_day}'
                  AND keep_alive_flag = 1
            ) b ON a.user_id = b.user_id;
        """))

        # ========== 3. æŸ¥è¯¢æ±‡æ€» ==========
        print("ğŸ“Š ç”Ÿæˆç•™å­˜åˆ†æç»“æœ")
        sql = f"""
            SELECT /*+ SET_VAR(query_timeout = 120000) */
                '{target_date}' AS event_date,
                e.variation,
                g.country,
                COUNT(DISTINCT n.user_id) AS active_users,
                COUNT(DISTINCT CASE WHEN n.is_d1 = 1 THEN n.user_id END) AS d1
            FROM tmp_active_user_with_d1 n
            JOIN tmp_user_variation_country e ON n.user_id = e.user_id
            LEFT JOIN flow_event_info.tbl_wide_user_active_geo_daily g
                ON n.user_id = g.user_id AND g.event_date = '{target_date}'
            GROUP BY e.variation, g.country;
        """
        df = pd.read_sql(text(sql), conn)

    # ========== 4. åå¤„ç† ==========
    if df is not None and not df.empty:
        df["country"] = df["country"].replace("", "unknown").fillna("unknown")
        df["retention_d1"] = df["d1"] / df["active_users"].replace(0, 1)
        df["dt"] = target_date
        df["experiment_id"] = experiment_id
        return df
    else:
        print(f"âš ï¸ æ²¡æœ‰ç”Ÿæˆæ•°æ®ï¼š{target_date}")
        return pd.DataFrame()


# ===== æ‰¹é‡æ‰§è¡Œå¹¶æ±‡æ€»å†™å…¥ CSV =====
def batch_retention_analysis(experiment_id, start_date, end_date):
    engine = get_engine()
    os.makedirs("daily_retention", exist_ok=True)
    current = datetime.strptime(start_date, "%Y-%m-%d")
    end = datetime.strptime(end_date, "%Y-%m-%d")

    all_results = []

    while current <= end:
        target_date = current.strftime("%Y-%m-%d")
        try:
            df = run_retention_analysis(engine, experiment_id, target_date)
            if not df.empty:
                all_results.append(df)
        except Exception as e:
            print(f"âŒ {target_date} å¤„ç†å¤±è´¥ï¼š{e}")
        current += timedelta(days=1)

    if all_results:
        final_df = pd.concat(all_results, ignore_index=True)
        final_df = final_df[["dt", "experiment_id", "variation", "country", "active_users", "d1", "retention_d1"]]
        output_path = f"daily_retention/retention_by_country_active_{experiment_id}_{start_date.replace('-', '')}_{end_date.replace('-', '')}.csv"
        final_df.to_csv(output_path, index=False)
        print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²å†™å…¥ï¼š{output_path}")
    else:
        print("âš ï¸ æ²¡æœ‰ä»»ä½•æ•°æ®ç”Ÿæˆ")


# ===== å¯åŠ¨ä»»åŠ¡ =====
if __name__ == "__main__":
    experiment_id = "app_sensitive_image_exp"
    batch_retention_analysis(experiment_id, start_date="2025-04-02", end_date="2025-04-13")
